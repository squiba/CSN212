#+TITLE: Intro to Algorithms: CLRS
#+OPTIONS: toc:nill
#+DATE: 14/02/2017
* COMMENT Part1: Foundations
* Chapter2: Getting Started
** Merge Sort 

*** Algorithm
#+BEGIN_SRC python 
  # To sort A call MergeSort(A,0,len(A)-1)
  def MergeSort(A,p,r):
      if p<r:
          q=(p+r)/2
          MergeSort(A,p,q)
          MergeSort(A,q+1,r)
          Merge(A,p,q,r)

#+END_SRC

#+BEGIN_SRC python
  # function to merge A[p:q] and A[q+1:r]
  def Merge(A,p,q,r):
      n1 = q-p+1
      n2 = r-q
      L = [None]*(n1+1)
      R = [None]*(n2+1)
      for i in range(0,n1,1):
          L[i]=A[p+i]
      for j in range(0,n2,1):
          R[j]=A[q+j+1]
      L[n1] = float("inf")
      R[n2] = float("inf")
      i=0
      j=0
      for k in range(p,r+1,1):
          if L[i] <= R[j]:
              A[k]=L[i]
              i=i+1
          else:
              A[k]=R[j]
              j=j+1
#+END_SRC

*** Correctness
Defining the following loop invariant: \\
At the start of each iteration of the for loop of lines 22-28,
1. the subarray $A[p..k-1]$ contains the $k-p$ smallest elements of $L[0..n1]$ and $R[0..n2]$, in sorted order.
2. Moreover, $L[i]$ and $R[j]$ are the smallest elements of their arrays that have not been copied back into $A$. \\

*Initialization:* 1) $k=p$,  A[p..k-1] is empty: k-p=0 smallest elements.
2)$i=j=1$, L[1] and R[1] are the smallest elements that have not copied back to A. \\

*Induction:* Suppose L[i]\leq R[j], then L[i] is the smallest element not yet copied back in A and line 24 do that. 
As subarray A[p..k-1] contained the k-p smallest element initially now it has k-p+1 smallest elements and then k is incremented
to hold the 1st point of loop invariant. i is also incremented which maintain 2nd point of loop invariant. Similarly is L[i]>R[i]
appropriate actions are taken to maintain the loop invariant. \\

*Termination:* k=r+1, at this point A[p..r] is sorted and contain r+1-p smallest element of L and R which are of A. So now 
we have sorted array A.

*** Analyzing Merge Sort
There are three steps:
1) Divide: O(1)
2) Conquer: $2T(n/2)$, solving two sub-problems of n/2 sizes.
3) Combine: O(n), In Merge for loop from lines 22 to 28 runs n times.
which gives \\
\begin{equation}
T(n) = \begin{cases}
\theta(1), & \text{if $n=1$}.\\
2T(n/2) + \theta(n), & \text{if $n>1$}.
\end{cases}
\end{equation}

using recursion tree [[./img/mergesort.png]]

thus T(n) = O(n \log n).
* Chapter4: Divide and Conquer
** Maximum Subarray Problem
#+NAME: Find_Max_Crossing_Subarray
#+BEGIN_SRC python
  def Find_Max_Crossing_Subarray(A,low,mid,high):
      left_sum=-float("inf")
      sum=0
      for i in range(mid,low-1,-1):
          sum=sum+A[i]
          if sum>left_sum:
              left_sum=sum
              max_left=i
      right_sum=-float("inf")
      sum=0
      for j in range(mid+1,high+1,1):
          sum=sum+A[j]
          if sum>right_sum:
              right_sum=sum
              max_right=j
      return(max_left,max_right,left_sum+right_sum)
#+END_SRC

#+NAME: Find_Max_Subarray
#+BEGIN_SRC python :exports code :noweb-ref Find_Max_Subarray -n
  def Find_Max_Subarray(A,low,high):
      if high==low:
          return (low,high,A[low]) # base case
      else:
          mid = (low+high)/2
          (left_low,left_high,left_sum)=Find_Max_Subarray(A,low,mid)
          (right_low,right_high,right_sum)=Find_Max_Subarray(A,mid+1,high)
          (cross_low,cross_high,cross_sum)=Find_Max_Crossing_Subarray(A,low,mid,high)

          if left_sum>=right_sum and left_sum>=cross_sum:
              return (left_low,left_high,left_sum)
          elif right_sum>=left_sum and right_sum>=cross_sum:
              return (right_low,right_high,right_sum)
          else:
              return (cross_low,cross_high,cross_sum)
#+END_SRC

Analyzing the divide and conquer algorithm:

\begin{equation}
T(n) = \begin{cases}
\theta(1), & \text{if $n=1$}.\\
2T(n/2) + \theta(n), & \text{if $n>1$}.
\end{cases}
\end{equation}

** Substitution method for solving recurrences
1) Guess the form of the solution
2) Verify: Show the solution works
3) Find constants mathematical induction
we substitute the guessed solution for the problem when applying the inductive hypothesis to smaller values; hence the name substitution method.

For solving the above equation we guess that the solution is $T(n)=\theta(n \log n)$.Assume this solution holds for all $m < n$ , in particular for $m = n/2$ , yielding
T(n/2) \leq $c n/2 \log n/2$. Substituting in the recurrence equation gives

\begin{align*}
T(n) &\leq 2(c n/2 \log n/2) + n \\
&\leq c n \log n/2 + n \\
&= cn \log n - cn \log2 + n \\
&= cn \log n - cn + n \\
&\leq cn \log n & \forall c \geq 1
\end{align*}

** Recurrence tree method for solving recurrences

** Master method for solving recurrences
of the form
\begin{align*}
T(n) = aT(n/b) + f(n), & a\geq1 \& b>1
\end{align*}

* COMMENT Part2: Sorting and Order Statistics
* Chapter6: Heap Sort
$ O (n \log n)$, in place

** Heap
The heap data structure is an array object that we can view as a near complete binary tree.
Each node of the tree corresponds to an element in the array. Root of the tree is A[1] and
#+BEGIN_SRC python
  def Parent(i):
      return i/2
  def Left(i):
      return 2*i
  def Right(i):
      return 2*i+1

#+END_SRC
*max-heap* : A[Parent(i)] \geq A[i] \\
*min-heap* : A[Parent(i)] \leq A[i]
** Algorithm
*Max-Heapify* : in worst case when bottom level is half filled
\begin{equation}
T(n) <= T(2n/3) + \theta (1)
\end{equation}
whose solution by master theorem case 2 is $ O( \log n) $
#+BEGIN_SRC python
  # maintain heap property for one violation at i
  def Max_Heapify(A,i,heap_size=None):
      if heap_size == None:
          heap_size = len(A)
      l=Left(i)
      r=Right(i)
      if l<=heap_size and A[l]>A[i]:
          largest = l
      else:
          largest = i
      if r<=heap_size and A[r]>A[largest]:
          largest = r
      if largest != i:
          A[i],A[largest] = A[largest], A[i]
          Max_Heapify(A,largest)

#+END_SRC

*Build-Max-Heap* : 
$$ \sum_{c=0}^{\log n} c*n/2^c+1 = c*n / 2 $$
\implies $ O(n) $
#+BEGIN_SRC python
  def Build_Max_Heap(A):
      for i in range(len(A)/2,1,-1):
          Max_Heapify(A,i)

#+END_SRC

*Heap-Sort* :
#+BEGIN_SRC python
  def HeapSort(A):
      Build_Max_Heap(A)
      heap_size = len(A)
      for i in range(len(A),1,-1):
          index=i-1
          A[1],A[index] = A[index],A[1]
          heap_size -=1
          Max_Heapify(A,index,heap_size)

#+END_SRC

* Chapter7: Quick Sort
** Algorithm
#+BEGIN_SRC python -n
  # To sort A call (A,0,len(A)-1)
  def QuickSort(A,p,r):
      if p<r:
          q=Partion(A,p,r)
          QuickSort(A,p,q-1)
          QuickSort(A,q+1,r)
#+END_SRC

#+BEGIN_SRC python +n
  # function to partion A and place pivot at an appropriate position
  def Partion(A,p,r):
      x=A[r]
      i=p-1
      for j in range(p,r):
          if A[j]<=x:
              i=i+1
              A[i],A[j]=A[j],A[i]
      A[i+1],A[r] = A[r],A[i+1]
      return i+1
#+END_SRC

** Correctness
loop invariant for line 11-15, let k be an index 0 \leq k \leq n then
1) if p \leq k \leq i then A[k] \leq x
2) if i+1 \leq k \leq j-1 then A[k] > x
3) if k=r then A[k] = x \\

*Initialization:* initially i=p-1 and j=p then 1) no values between p and i implies A[k] \leq x, similarly between i+1 and j-1 
no values implies A[k]>x , Also $A[r]=x$.

*Induction:* [[./img/quicksort_correctness.png]]
a) Figure (a) shows what happens when A[j] > x;the only action in the loop is to increment j . After j is incremented, condition 2
holds for A[j-1] and all other entries remain unchanged.\\
b) Figure (b) shows what happens when A[j] \leq x; the loop increments i, swaps A[i] and A[j], and then increments j . 
Because of the swap, we now have that A[i] \leq x, and condition 1 is satisfied. Similarly, we also have that A[j]> x, since the
item that was swapped into A[j-1]is, by the loop invariant, greater than x.

*Termination:* when j=r and at that point. 1) A[p...i] \leq x. 2) $A[i+1...r-1]>x$. 3) A[r]=x.At last we exchange pivot with 
leftmost element greater then x and move it to its correct position.

** Analyzing Quick Sort
Running time of Partition(A,p,r) = \theta (n) as for loop runs for n=r-p+1 times. \\
1 Worst case partitioning : Sub-problems have 0 and (n-1) size.
\begin{align*}
T(n) = T(n-1) + \theta(n)
\end{align*}
whose solution is
\begin{equation}
T(n) = \theta ( {n}^2 )
\end{equation}
\\
2 Best Case Partitioning: when both problems have size of n/2 then
\begin{align*}
T(n)=2T(n/2) + \theta (n)
\end{align*}
whose solution by master theorem is
\begin{equation}
T(n) = O (n \log n)
\end{equation}
\\
3 Average Case:
if there is some partitioning lets us say in 9/10 and 1/10 then
[[./img/quicksort_analyze.png]]
which gives
\begin{equation}
T(n) = O (n \log n)
\end{equation}


* COMMENT Part3: Data Structures
* Chapter12: Binary Search Tree
A binary search tree is a binary tree such that for any node x, 
1. x.left.key <= x.key
2. x.right.key >=x

*Sort* To print the keys in sorted order
#+BEGIN_SRC python
  def Inorder_Tree_Walk(root):
      if Root != None:
          Inorder_Tree_Walk(root.left)
          print root.key
          Inorder_Tree_Walk(root.right)
#+END_SRC

*Search*
#+BEGIN_SRC python
  def Tree_Search(root,value):
      if root==None or value ==root.key:
          return root
      if value<root.key:
          return Tree_Search(root.left,value)
      else:
          return Tree_Search(root.right,value)
#+END_SRC

#+BEGIN_SRC python
  def Iterative_Tree_Search(root,value):
      while root!=None and value!=root.key:
          if value<root.key:
              root=root.left
          else:
              root = root.right
      return root

#+END_SRC

*Minimum and Maximum*
#+BEGIN_SRC python
  def Tree_Minimum(root):
      while root.left != None:
          root=root.left
      return root
  def Tree_Maximum(root):
      while root.right != None:
          root = root.right
      return root

#+END_SRC
* Chapter13: Augumented Data Str

* COMMENT Part4: Advanced Design and Analysis Techniques
* Chapter15: Dynamic Programming
** Elements of dynamic Programming
1. *optimal substructure*: optimal solution to the problem contains within it the optimal solution to subproblems.
   - how amny subproblems an optimal solution has
   - how many choices to determine which subproblem(s) to use
2. *overlapping subproblems*: 

** Rod Cutting
Brute-force solution
#+BEGIN_SRC python
  def CUT_ROAD(p,n):
      # p =price table, n= len of road
      # return q= max rewards that can be obtained from a road of given length n by cutting the road
      if n==o:
          return 0

      q=-float('inf')
      for i in range(1,n+1):
          q=max(q,p[i] + CUT_ROAD(p,n-i))
      return q

#+END_SRC

#+BEGIN_SRC python
  def Memoized_Cut_Road(p,n):
    

#+END_SRC

** Matrix-Chain multiplication
#+BEGIN_SRC python
  def Matrix_Chain_Order(p):
      # p = array of size n+1 having the dimensions of matrices to multiply
      n=len(p)-1
      m = [[ float('inf') for j in range(n)] for i in range(n)]
      s = [[ 0 for j in range(n)] for i in range(n)]

      for i in range(n):
          m[i][i] = 0

      for l in range(2,n+1):
          #l = chain length
          for i in range(n-l+1):
              j=i+l-1

              for k in range(i,j):
                  q=m[i][k]+m[k+1][j]+p[i]*p[k+1]*p[j+1]
                  if q<m[i][j]:
                      m[i][j]=q
                      s[i][j]=k+1
      return m,s
  p=[30,35,15,5,10,20,25]
  return Matrix_Chain_Order(p)
#+END_SRC

#+RESULTS:
| (0 15750 7875 9375 11875 15125) | (inf 0 2625 4375 7125 10500) | (inf inf 0 750 2500 5375) | (inf inf inf 0 1000 3500) | (inf inf inf inf 0 5000) | (inf inf inf inf inf 0) |
| (0 1 1 3 3 3)                   | (0 0 2 3 3 3)                | (0 0 0 3 3 3)             | (0 0 0 0 4 5)             | (0 0 0 0 0 5)            | (0 0 0 0 0 0)           |

